{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary learning step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_shrink(\n",
    "        x:np.array, \n",
    "        lambda_:float\n",
    "):\n",
    "    \n",
    "    return np.sign(x) * np.maximum(0, np.abs(x) - lambda_)\n",
    "\n",
    "def sparse_coding_IPM(\n",
    "        A:np.array, \n",
    "        D:np.array, \n",
    "        Lambda1:float, \n",
    "        Lambda2:float, \n",
    "        eps = 1e-4, \n",
    "        max_iter = 10\n",
    "):\n",
    "\n",
    "    X = np.random.randn(D.shape[1], A.shape[1])\n",
    "\n",
    "    loss = np.linalg.norm(A - D @ X)**2 + Lambda1 * np.linalg.norm(X, ord = 1)\n",
    "    iters = 0\n",
    "\n",
    "    # Useful precomputations\n",
    "    B = D.T @ A\n",
    "    C = D.T @ D\n",
    "\n",
    "    # Optimal stepsize\n",
    "    mu = np.real(1 /np.max(np.real(np.linalg.eigvals(C))))\n",
    "    \n",
    "    while loss > eps and iters < max_iter:\n",
    "\n",
    "        m = np.mean(X, axis = 0)\n",
    "        M = np.tile(m.reshape(-1, 1), X.shape[0]).transpose(1,0)\n",
    "\n",
    "        X_hat = X + mu*((B - C @ X) + Lambda2 * (X - M))\n",
    "\n",
    "        X = soft_shrink(X_hat, Lambda1 * mu)\n",
    "\n",
    "        # Loss tracking\n",
    "        loss = np.linalg.norm(A - D @ X)**2 + Lambda1 * np.linalg.norm(X, ord = 1) + Lambda2 * np.linalg.norm(X - M)\n",
    "        iters += 1\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q(\n",
    "        D:np.array, \n",
    "        W:np.array, \n",
    "        C:int,\n",
    "        n:int\n",
    "):\n",
    "\n",
    "    S, N = D.shape \n",
    "    Q = np.zeros((S, S))  \n",
    "    \n",
    "    for m in np.arange(N)[np.arange(N) != n]:\n",
    "\n",
    "        inner_sum = 0\n",
    "        \n",
    "        for j in range(C):\n",
    "\n",
    "            for l in np.arange(C)[np.arange(C) != j]:\n",
    "\n",
    "                inner_sum += W[m,j] * W[n,l]\n",
    "\n",
    "        Q += np.outer(D[:,m], D[:,m]) * inner_sum\n",
    "\n",
    "    return Q\n",
    "\n",
    "def dictionary_update(\n",
    "        A:dict, \n",
    "        X:dict, \n",
    "        W:np.array, \n",
    "        D:np.array, \n",
    "        C:int, \n",
    "        Lambda3:float\n",
    "):\n",
    "\n",
    "    Y = {c:None for c in range(C)}\n",
    "    \n",
    "    for c in range(C):\n",
    "\n",
    "        Y[c] = np.diag(W[:,c]) @ X[c] \n",
    "\n",
    "    Y = np.concatenate(list(Y.values()), axis = 1)\n",
    "    A = np.concatenate(list(A.values()), axis = 1)\n",
    "    \n",
    "    G = Y @ Y.T\n",
    "    L = A @ Y.T\n",
    "\n",
    "    N = D.shape[1]\n",
    "    D_hat = np.zeros_like(D)\n",
    "\n",
    "    for n in range(N):\n",
    "\n",
    "        Q = get_Q(D, W, C, n)\n",
    "        u = np.linalg.inv(L[n,n] * np.eye(Q.shape[0]) + Lambda3 * np.diag(np.diag(Q))) @ (L[:,n] - D @ G[:,n] - Lambda3 * Q @ D[:,n])\n",
    "        D_hat[:,n] = u / np.linalg.norm(u)\n",
    "\n",
    "    return D_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DictionaryLearning(\n",
    "        A:dict, \n",
    "        W:np.array,\n",
    "        D:np.array,\n",
    "        C:int, \n",
    "        Lambda1:float, \n",
    "        Lambda2:float, \n",
    "        Lambda3:float, \n",
    "        eps = 1e-2, \n",
    "        MAXITER=10\n",
    "):\n",
    "\n",
    "    for _ in range(MAXITER):\n",
    "        \n",
    "        # Sparse coding step\n",
    "\n",
    "        X = {c: None for c in range(C)}\n",
    "\n",
    "        for c in range(C):\n",
    "\n",
    "            X[c] = sparse_coding_IPM(A[c], D, Lambda1, Lambda2)\n",
    "\n",
    "        # Dictionary update step\n",
    "\n",
    "        D = dictionary_update(A, X, W, D, C, Lambda3)\n",
    "\n",
    "    return X, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent matrix update step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_vector_update(\n",
    "        X:dict,\n",
    "        k:int, \n",
    "        N:int,\n",
    "        C:int,\n",
    "        delta:float,\n",
    "        sigma:float,\n",
    "        Lambda3:float,\n",
    "        A:np.array,\n",
    "        D:np.array,\n",
    "        W:np.array,\n",
    "        MAXITER = 10,\n",
    "        ):\n",
    "    \n",
    "    b = np.zeros(N)\n",
    "    w = np.copy(W[:,k])\n",
    "\n",
    "    for n in range(N):\n",
    "\n",
    "        b_ = 0\n",
    "\n",
    "        for m in [i for i in range(N) if i != n]:\n",
    "            \n",
    "            d_mn = np.dot(D[:,m], D[:,n])**2\n",
    "            inner_sum = 0\n",
    "\n",
    "            for j in [i for i in range(C) if i != k]:\n",
    "\n",
    "                inner_sum += W[j,m]\n",
    "            \n",
    "            b_ += d_mn * inner_sum\n",
    "        \n",
    "        b[n] = b_\n",
    "    \n",
    "    a = A.flatten()\n",
    "    B = []\n",
    "    for n in range(N):\n",
    "        B.append(np.outer(D[:,n],X[n,:])) \n",
    "    \n",
    "    R = np.concatenate([B_.flatten() for B_ in B])\n",
    "\n",
    "    iter = 0\n",
    "\n",
    "    while iter < MAXITER:\n",
    "\n",
    "        tau_0 = w - R.T @ (R @ (w - a) + Lambda3 * b) / sigma\n",
    "        tau_1 = tau_0 - (np.sum(tau_0 - delta))/N\n",
    "        tau_2 = np.maximum(0, tau_1)\n",
    "        w = tau_2 /(np.sum(tau_2) * delta)\n",
    "\n",
    "        iter += 1\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LatentMatrixLearning(\n",
    "        X:dict,\n",
    "        N:int,\n",
    "        C:int,\n",
    "        delta:float,\n",
    "        sigma:float,\n",
    "        Lambda3:float,\n",
    "        A:np.array,\n",
    "        D:np.array,\n",
    "        W:np.array,\n",
    "        MAXITER = 100,\n",
    "):\n",
    "\n",
    "    W_hat = np.zeros_like(W)\n",
    "\n",
    "    for k in range(C):\n",
    "        W_hat[:,k] = latent_vector_update(X, k, N, C, delta, sigma, Lambda3, A, D, W, MAXITER)\n",
    "\n",
    "    return W_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDL(        \n",
    "        N:int,\n",
    "        C:int,\n",
    "        delta:float,\n",
    "        sigma:float,\n",
    "        Lambda1:float,\n",
    "        Lambda2:float,\n",
    "        Lambda3:float,\n",
    "        A:np.array,\n",
    "        MAXITER = 100\n",
    "        ):\n",
    "    \n",
    "    # Dictionary initialization\n",
    "    D = np.random.randn(A[0].shape[0], N)\n",
    "    D = np.apply_along_axis(lambda x: x/np.linalg.norm(x), axis = 0, arr = D)\n",
    "    \n",
    "    # Initialize the latent matrix\n",
    "    W = np.random.randn(N, C)\n",
    "    W = np.maximum(0, W)\n",
    "\n",
    "    for _ in tqdm(range(MAXITER)):\n",
    "\n",
    "        # Dictionary update\n",
    "        X, D = DictionaryLearning(A, W, D, C, Lambda1, Lambda2, Lambda3)\n",
    "\n",
    "        # Latent matrix update\n",
    "        W = LatentMatrixLearning(X, N, C, delta, sigma, Lambda3, A, D, W)\n",
    "\n",
    "    return D, W     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_data = np.concatenate([x_train, x_test])\n",
    "y_data = np.concatenate([y_train, y_test])\n",
    "\n",
    "# Flattening images to vectors\n",
    "x_data = x_data.reshape(x_data.shape[0], -1)  \n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "mnist_dict = {class_label: None for class_label in range(10)}\n",
    "\n",
    "# Loop through each class (0 to 9) and collect the corresponding flattened images\n",
    "for class_label in range(10):\n",
    "    mnist_dict[class_label] = x_data[np.where(y_data == class_label)[0][:100]].transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "C = 10\n",
    "\n",
    "D, W = LDL(N, C, 1, 0.01, 0.15, 0.25, 0.35, mnist_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
