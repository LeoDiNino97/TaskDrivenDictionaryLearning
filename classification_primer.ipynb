{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dataset of signals and binary labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticSignals():\n",
    "    def __init__(self, A, B, n, m, s, s_sigma = 0.5, eps_sigma = 0.01, size = 1000, batch_size = 512):\n",
    "        \n",
    "        self.n = n          # Number of samples in the original signal\n",
    "        self.m = m          # Number of samples through the linear transformation\n",
    "\n",
    "        self.size = size    # Size of the dataset\n",
    "\n",
    "        self.alpha = torch.zeros(self.size, self.n)          # Underlying sparse vector \n",
    "        self.x = torch.zeros(self.size, self.m)              # Observation\n",
    "        self.y = torch.zeros(self.size)                     # Binary label \n",
    "\n",
    "        self.A = A          # Matrix for the linear observation - Label 0\n",
    "        self.B = B          # Matrix for the linear observation - Label 1\n",
    "        self.s = s          # Sparsity of the signal\n",
    "\n",
    "        self.s_sigma = s_sigma\n",
    "        self.eps_sigma = eps_sigma\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Generating the dataset\n",
    "        self.set_data()\n",
    "\n",
    "\n",
    "    def set_tuple(self, i):\n",
    "\n",
    "        # Generating random sparsity in the canonic basis of the original signal\n",
    "        idxs = np.random.choice(self.m, self.s, replace=False)\n",
    "        peaks = np.random.normal(scale=self.s_sigma, size = self.s)\n",
    "        y_ = np.random.choice([-1,1])\n",
    "        \n",
    "        # Generating the original signal and its corrupted observations according to a label\n",
    "        self.alpha[i, idxs] = torch.from_numpy(peaks).to(self.alpha)\n",
    "        self.x[i,:] = (self.A * (y_ == 1) + self.B * (y_ == 0)) @ self.alpha[i,:] + np.random.normal(scale=self.eps_sigma, size=self.m)\n",
    "        self.y[i] = y_\n",
    "\n",
    "    def set_data(self):\n",
    "        for i in range(self.size):\n",
    "            self.set_tuple(i)\n",
    "    \n",
    "    \n",
    "    def set_loader(self):\n",
    "\n",
    "        # We need tuples observation/label\n",
    "        return Data.DataLoader(dataset = Data.TensorDataset(self.x, self.y),\n",
    "                               batch_size = self.batch_size,\n",
    "                               shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions for the signal, its sparsity and its observation\n",
    "\n",
    "m_ = 150\n",
    "n_ = 200\n",
    "s_ = 4\n",
    "\n",
    "# Measurement matrices\n",
    "A_ = torch.randn(m_,n_)\n",
    "A_ /= torch.norm(A_, dim=0)\n",
    "\n",
    "B_ = torch.randn(m_,n_)\n",
    "B_ /= torch.norm(A_, dim=0)\n",
    "\n",
    "# Building a training set and a test set \n",
    "train_set = SyntheticSignals(A = A_,\n",
    "                             B = B_,\n",
    "                             n = n_,\n",
    "                             m = m_,\n",
    "                             s = s_,\n",
    "                             size = 800).set_loader()\n",
    "\n",
    "test_set = SyntheticSignals(A = A_,\n",
    "                            B = B_,\n",
    "                            n = n_,\n",
    "                            m = m_,\n",
    "                            s = s_,\n",
    "                            size = 200).set_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDDL(nn.Module):\n",
    "    def __init__(self, D, K, in_, hidden_, Lambda, T = 100, t_0 = 1, LR = 5e-03):\n",
    "        super().__init__()\n",
    "\n",
    "        # Dictionary initialization \n",
    "        self.D = D\n",
    "\n",
    "        # Assumed sparsity\n",
    "        self.K = K\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.Lambda = Lambda\n",
    "        self.t_0 = t_0\n",
    "        self.T = T\n",
    "        self.LR = LR\n",
    "\n",
    "        # Define the neural architecture\n",
    "        self.fc1 = nn.Linear(in_, hidden_)\n",
    "        self.fc2 = nn.Linear(hidden_, 1)\n",
    "\n",
    "        # Define the optimization utilities \n",
    "        self.criterion = nn.BCELoss()  \n",
    "\n",
    "    def forward(self, alpha):\n",
    "\n",
    "        alpha = F.relu(self.fc1(alpha))\n",
    "        alpha = torch.sigmoid(self.fc2(alpha))\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def OMP(self, x):\n",
    "        S = []\n",
    "        alpha = torch.zeros(self.D.shape[1])\n",
    "        iters = 0\n",
    "        R = x\n",
    "\n",
    "        while iters < self.K:\n",
    "\n",
    "            # Retrieve the maximum correlation between atoms and residuals of the previous iteration\n",
    "            S.append(torch.argmax(torch.abs(torch.matmul(self.D.t(), R))).item())\n",
    "\n",
    "            dic = self.D[:, S]\n",
    "            x_S = torch.linalg.pinv(dic.t() @ dic) @ dic.t() @ self.D\n",
    "            alpha[S] = x_S\n",
    "\n",
    "            # Update the residuals\n",
    "            R = x - torch.matmul(self.D, alpha)\n",
    "            iters += 1\n",
    "\n",
    "        return torch.tensor(alpha, requires_grad=True)\n",
    "\n",
    "    def activeSet(alpha):\n",
    "\n",
    "        return torch.nonzero(alpha).squeeze()\n",
    "    \n",
    "    def projD(self):\n",
    "\n",
    "        # We constrain the atoms to have norm equal to one\n",
    "        def projCol(d):\n",
    "\n",
    "            return d / torch.max(torch.tensor(1.0), torch.linalg.norm(d))\n",
    "\n",
    "        return torch.stack([(projCol(D[:, i])) for i in range(D.shape[1])], dim=1)\n",
    "    \n",
    "    def update(self, x, y, t):\n",
    "\n",
    "        # Compute the sparse approximation and its non-zero entries set\n",
    "        alpha = self.OMP(x)\n",
    "        L = self.activeSet(alpha)\n",
    "\n",
    "        # Forward this sparse feature vector enabling gradient computation with respect to the sparse vector\n",
    "        y_hat = self.forward(alpha)\n",
    "\n",
    "        # Loss computation and backpropagation\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Define the support vector for the D-gradient computation \n",
    "        beta = torch.zeros_like(alpha)\n",
    "        dic = self.D[:,L] \n",
    "        beta[L] = torch.matmul(torch.linalg.inv(torch.matmul(dic.t(), dic)), alpha.grad[L])\n",
    "\n",
    "        ##########################\n",
    "        ### OPTIMIZATION PHASE ###\n",
    "        ##########################\n",
    "\n",
    "        # Learning rate heuristic\n",
    "        LR = torch.min(self.LR, self.LR * self.t_0/t)\n",
    "\n",
    "        # Gradient descent for the model parameters\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                param -= LR * param.grad\n",
    "\n",
    "        # Projected gradient descent for the dictionary \n",
    "        with torch.no_grad():\n",
    "            self.D = self.projD(self.D - LR * ( - self.D @ beta @ alpha.t() + (x - self.D @ alpha) @ beta.t()))\n",
    "\n",
    "    def trainLoop(self, train_set):\n",
    "\n",
    "        # Set to train mode\n",
    "        self.train()\n",
    "\n",
    "        # Main loop\n",
    "        for t in range(self.T):\n",
    "            idx = torch.randint(0, len(train_set.dataset), (1,))\n",
    "            x, y = train_set.dataset.__getitem__(idx)\n",
    "\n",
    "            self.update(x, y, t)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
